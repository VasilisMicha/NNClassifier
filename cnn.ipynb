import numpy as np
import torch
from torch import optim
from torch import nn
import torch.nn.functional as F
from data import DataPreprocessor
from torch.optim.lr_scheduler import ReduceLROnPlateau


class CNN(nn.Module):
    def __init__(self, in_channels, num_classes):
        """
        Building blocks of convolutional neural network.

        Parameters:
            * in_channels: Number of channels in the input image (for grayscale images, 1)
            * num_classes: Number of classes to predict. In our problem, 10 (i.e digits from  0 to 9).
        """
        super(CNN, self).__init__()

        # 1st convolutional layer
        self.conv1 = nn.Conv2d(
            in_channels=in_channels, out_channels=32, kernel_size=3, padding=1
        )
        # Max pooling layer
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        # 2nd convolutional layer
        self.conv2 = nn.Conv2d(
            in_channels=32, out_channels=64, kernel_size=3, padding=1
        )
        self.conv3 = nn.Conv2d(
            in_channels=64, out_channels=128, kernel_size=3, padding=1
        )
        # Fully connected layer
        self.fc1 = nn.Linear(128 * 8 * 8, num_classes)

    def forward(self, x):
        """
        Define the forward pass of the neural network.

        Parameters:
            x: Input tensor.

        Returns:
            torch.Tensor
                The output tensor after passing through the network.
        """
        x = F.relu(self.conv1(x))  # Apply first convolution and ReLU activation
        x = self.pool(x)  # Apply max pooling
        x = F.relu(self.conv2(x))  # Apply second convolution and ReLU activation
        x = self.pool(x)  # Apply max pooling
        x = F.relu(self.conv3(x))
        x = torch.flatten(x)  # Flatten the tensor
        x = self.fc1(x)  # Apply fully connected layer
        return x


def classes_to_weights(ytrain, nc=10):
    classes = np.array(ytrain).astype(int)
    weights = np.bincount(classes, minlength=nc)
    weights[weights == 0] = 1
    weights = 1 / weights
    weights /= weights.sum()
    return torch.Tensor(weights)


def calculate_accuracy(X, y):
    length = len(X)
    count = 0
    for i in range(length):
        data = torch.squeeze(X[i]).to(device)
        targets = y[i].to(device)
        outputs = model(data)
        _, pred = torch.topk(outputs, 1)
        pred = torch.squeeze(pred)
        if pred == y[i]:
            count += 1

    print(f"Accuracy: {count/length}")


def validate_model(model, X, y, criterion, device):
    model.eval()  # CRITICAL: Sets Dropout and BatchNorm to evaluation mode
    val_loss = 0.0
    count = 0

    length = len(X)
    with torch.no_grad():  # CRITICAL: Disables gradient calculation to save time/memory
        for i in range(length):
            data = torch.squeeze(Xvalid[i]).to(device)
            targets = yvalid[i].to(device)
            outputs = model(data)
            loss = criterion(outputs, targets)
            val_loss += loss.item()

            _, pred = torch.topk(outputs, 1)
            pred = torch.squeeze(pred)
            # print(f"{pred}-{y[i]}", end=",")
            if pred == y[i]:
                count += 1
    avg_val_loss = val_loss / length
    val_accuracy = 100 * count / length

    model.train()  # IMPORTANT: Restore the model to training mode after validation
    return avg_val_loss, val_accuracy


preprocessor = DataPreprocessor()
preprocessor.include_validation()
preprocessor.to_tensors()
Xtrain, ytrain, Xtest, ytest, Xvalid, yvalid = preprocessor.return_data()
Xtrain = Xtrain[:3000, :]
ytrain = ytrain[:3000]

device = "cpu"

model = CNN(in_channels=3, num_classes=10).to(device)

criterion = nn.CrossEntropyLoss(weight=classes_to_weights(ytrain))
optimizer = optim.Adam(model.parameters(), lr=0.0001)
scheduler = ReduceLROnPlateau(optimizer, mode="min", factor=0.1, patience=0)

length1 = len(Xtrain)
num_epochs = 15
for epoch in range(num_epochs):
    running_loss = 0.0

    for i in range(length1):
        data = torch.squeeze(Xtrain[i]).to(device)
        targets = ytrain[i].to(device)
        optimizer.zero_grad()
        scores = model(data)
        loss = criterion(scores, targets)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    print(
        f"Epoch [{epoch + 1}/{num_epochs}]----------------------------------------------------------------"
    )
    epoch_loss = running_loss / length1
    print(f"--- Epoch {epoch+1} FINISHED. Avg. Epoch Loss: {epoch_loss:.4f} ---")
    avg_val_loss, val_accuracy = validate_model(
        model, Xvalid, yvalid, criterion, device
    )
    current_lr = optimizer.param_groups[0]["lr"]
    scheduler.step(avg_val_loss)
    print(
        f"Epoch {epoch+1}: Validation Loss = {avg_val_loss:.4f}, Accuracy = {val_accuracy:.2f}%"
    )
    print(f"Epoch {epoch+1}: Learning Rate: {current_lr}")


print(calculate_accuracy(Xtrain, ytrain))
print(calculate_accuracy(Xtest, ytest))
